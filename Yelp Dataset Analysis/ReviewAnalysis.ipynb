{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change to any path\n",
    "path = 'yelp_academic_dataset_review.json'\n",
    "reviews = pd.read_json(path, lines=True, chunksize=10000)\n",
    "reviews_for_filter = pd.read_json(path, lines=True, chunksize=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "for r in reviews_for_filter:\n",
    "    subset = r\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1000, 9)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "subset.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "review_id              object\n",
       "user_id                object\n",
       "business_id            object\n",
       "stars                   int64\n",
       "useful                  int64\n",
       "funny                   int64\n",
       "cool                    int64\n",
       "text                   object\n",
       "date           datetime64[ns]\n",
       "dtype: object"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "subset.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review_id</th>\n",
       "      <th>user_id</th>\n",
       "      <th>business_id</th>\n",
       "      <th>stars</th>\n",
       "      <th>useful</th>\n",
       "      <th>funny</th>\n",
       "      <th>cool</th>\n",
       "      <th>text</th>\n",
       "      <th>date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>KU_O5udG6zpxOg-VcAEodg</td>\n",
       "      <td>mh_-eMZ6K5RLWhZyISBhwA</td>\n",
       "      <td>XQfwVwDr-v0ZS3_CbbE5Xw</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>If you decide to eat here, just be aware it is...</td>\n",
       "      <td>2018-07-07 22:09:11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>BiTunyQ73aT9WBnpR9DZGw</td>\n",
       "      <td>OyoGAe7OKpv6SyGZT5g77Q</td>\n",
       "      <td>7ATYjTIgM3jUlt4UM3IypQ</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>I've taken a lot of spin classes over the year...</td>\n",
       "      <td>2012-01-03 15:28:18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>saUsX_uimxRlCVr67Z4Jig</td>\n",
       "      <td>8g_iMtfSiwikVnbP2etR0A</td>\n",
       "      <td>YjUWPpI6HXG530lwP-fb2A</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Family diner. Had the buffet. Eclectic assortm...</td>\n",
       "      <td>2014-02-05 20:30:30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>AqPFMleE6RsU23_auESxiA</td>\n",
       "      <td>_7bHUi9Uuf5__HHc_Q8guQ</td>\n",
       "      <td>kxX2SOes4o-D3ZQBkiMRfA</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>Wow!  Yummy, different,  delicious.   Our favo...</td>\n",
       "      <td>2015-01-04 00:01:03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Sx8TMOWLNuJBWer-0pcmoA</td>\n",
       "      <td>bcjbaE6dDog4jkNY91ncLQ</td>\n",
       "      <td>e4Vwtrqf-wpJfwesgvdgxQ</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>Cute interior and owner (?) gave us tour of up...</td>\n",
       "      <td>2017-01-14 20:54:15</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                review_id                 user_id             business_id  \\\n",
       "0  KU_O5udG6zpxOg-VcAEodg  mh_-eMZ6K5RLWhZyISBhwA  XQfwVwDr-v0ZS3_CbbE5Xw   \n",
       "1  BiTunyQ73aT9WBnpR9DZGw  OyoGAe7OKpv6SyGZT5g77Q  7ATYjTIgM3jUlt4UM3IypQ   \n",
       "2  saUsX_uimxRlCVr67Z4Jig  8g_iMtfSiwikVnbP2etR0A  YjUWPpI6HXG530lwP-fb2A   \n",
       "3  AqPFMleE6RsU23_auESxiA  _7bHUi9Uuf5__HHc_Q8guQ  kxX2SOes4o-D3ZQBkiMRfA   \n",
       "4  Sx8TMOWLNuJBWer-0pcmoA  bcjbaE6dDog4jkNY91ncLQ  e4Vwtrqf-wpJfwesgvdgxQ   \n",
       "\n",
       "   stars  useful  funny  cool  \\\n",
       "0      3       0      0     0   \n",
       "1      5       1      0     1   \n",
       "2      3       0      0     0   \n",
       "3      5       1      0     1   \n",
       "4      4       1      0     1   \n",
       "\n",
       "                                                text                date  \n",
       "0  If you decide to eat here, just be aware it is... 2018-07-07 22:09:11  \n",
       "1  I've taken a lot of spin classes over the year... 2012-01-03 15:28:18  \n",
       "2  Family diner. Had the buffet. Eclectic assortm... 2014-02-05 20:30:30  \n",
       "3  Wow!  Yummy, different,  delicious.   Our favo... 2015-01-04 00:01:03  \n",
       "4  Cute interior and owner (?) gave us tour of up... 2017-01-14 20:54:15  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "subset.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "sortedSubset = subset.sort_values(by=['business_id'])\n",
    "x = subset['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make list of review text\n",
    "reviewText = []\n",
    "for index in enumerate(x):\n",
    "    reviewText.append(index[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000\n"
     ]
    }
   ],
   "source": [
    "print(len(reviewText))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Overview of Pros and Cons Analysis\n",
    "\n",
    "## Create a Filter\n",
    "- reviews sorted by business_id\n",
    "- combine the text of the reviews\n",
    "- remove stop words\n",
    "- count phrases\n",
    "- filter similar phrases\n",
    "\n",
    "## Potential Issues of using machine learning to identify positive and negative phrases\n",
    "- Although sentiment analysis is a solution to this task, using a model on individual phrases to identify if it is a positive or negative phrase may take an exponential computation time.\n",
    "- A band-aid solution is to review the most common phrases and manually filter relevant phrases and assign each phrase a pos or neg classification.\n",
    "- Two files will need to be created, one as mentioned before, to classify each filtered phrase as a pro or a con.\n",
    "- The second file maps phrases to a \"display\" phrase that will show on the webapp. For example, there can be two phrases that will be considered the same pro. \"Service is Good\" and \"Great service\" both mean the same thing and we don't want redundancy.\n",
    "- If there was more time to do this project, I would have tested a machine learning solution, but we'll use the manual solution for now.\n",
    "\n",
    "## Use the filter\n",
    "- group reviews by restaurant\n",
    "- get most common phrases\n",
    "- filter out phrases by the list made\n",
    "- save a list of pros and cons by business_id\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'each', 'other', 'some', 'such', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'will', 'just', 'don', 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\"]\n"
     ]
    }
   ],
   "source": [
    "#nltk.download('stopwords')\n",
    "print(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Following function is inspired by:\n",
    "https://dev.to/mattschwartz/quickly-find-common-phrases-in-a-large-list-of-strings-9in"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords = stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def get_common_phrases(texts, maximum_length=3, minimum_repeat=2) -> dict:\n",
    "    phrases = {}\n",
    "    \n",
    "    for text in texts:\n",
    "        # clean text\n",
    "        text = re.sub(r'[.!?,:;/\\-\\s]',' ', text)\n",
    "        text = re.sub(r'[\\\\|@#$&~%\\(\\)*\\\"]', '', text)\n",
    "        \n",
    "        words = text.split(' ')\n",
    "        # filter stopwords\n",
    "        words = [w for w in words if len(w) and w.lower() not in stopwords]\n",
    "        length = len(words)\n",
    "        \n",
    "        # phrases can only be maximum_length words long\n",
    "        size = length if length <= maximum_length else maximum_length\n",
    "        while size > 0:\n",
    "            pos = 0\n",
    "            # walk over sets of words\n",
    "            while pos + size <= length:\n",
    "                phrase = words[pos:pos+size]\n",
    "                phrase = tuple(w.lower() for w in phrase)\n",
    "                if phrase in phrases:\n",
    "                    phrases[phrase] += 1\n",
    "                else:\n",
    "                    phrases[phrase] = 1\n",
    "                pos += 1\n",
    "            size -= 1\n",
    "    # remove phrases that are less than the length set by minimum_repeat\n",
    "    phrases = {k: v for k, v in phrases.items() if v >= minimum_repeat}\n",
    "    # Loop through the dictionary of phrases and remove phrases that are too similar to each other.\n",
    "    longest_phrases = {}\n",
    "    keys = list(phrases.keys())\n",
    "    keys.sort(key=len, reverse=True)\n",
    "    for phrase in keys:\n",
    "        found = False\n",
    "        for l_phrase in longest_phrases:\n",
    "            intersection = set(l_phrase).intersection(phrase)\n",
    "            if len(intersection) == len(phrase):\n",
    "                difference = (phrases[phrase] - longest_phrases[l_phrase]) / longest_phrases[l_phrase]\n",
    "                if difference < 0.25:\n",
    "                    found = True\n",
    "                    break\n",
    "        if not found:\n",
    "            longest_phrases[phrase] = phrases[phrase]\n",
    "    # return a filtered dict of phrases\n",
    "    return longest_phrases\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Process for creating a custom filter for relevant phrases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze 1000 reviews to create the filter\n",
    "common_phrases = get_common_phrases(reviewText)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "keys = list(common_phrases.keys()) # businessID\n",
    "values = list(common_phrases.values()) # reviewText"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# sort common phrases by highest-> lowest\n",
    "sorted_value_index = np.argsort(values)\n",
    "sorted_value_index_desc= sorted_value_index[::-1]\n",
    "sortedPhrases = {keys[i]: values[i] for i in sorted_value_index_desc}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# perform some text cleaning before saving the output\n",
    "outputList = []\n",
    "for key in sortedPhrases:\n",
    "    txt = str(key)\n",
    "    txt = txt.replace(\"(\", \"\").replace(\")\",\"\").replace(\"'\",\"\").replace(\",\",\"\")\n",
    "    outputList.append(txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os.path\n",
    "save_path = 'output/commonPhrases.txt'\n",
    "with open(save_path, 'w') as f:\n",
    "    for word in outputList:\n",
    "        f.write(f\"{word}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Process for finding pros and cons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read filter list\n",
    "filterFile = 'output/combinePhrases.txt'\n",
    "mappedFile = 'output/uniqueProsandCons.txt'\n",
    "mapPhrases = {}\n",
    "with open(filterFile) as file:\n",
    "    for line in file:\n",
    "        x = line.split(',')\n",
    "        mapPhrases[x[0]] = x[1].strip('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "mapValue = {}\n",
    "with open (mappedFile) as file:\n",
    "    for line in file:\n",
    "        x = line.split(',')\n",
    "        mapValue[x[0]] = x[1].strip('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Spark Session\n",
    "spark = SparkSession.builder.master(\"local\").appName(\"findProsAndCons\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# group reviews by business_id\n",
    "reviewKeys = sortedSubset['business_id']\n",
    "reviewValues = sortedSubset['text']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pair busines_id with text\n",
    "reviewPairs = []\n",
    "for idx, key in enumerate(reviewKeys):\n",
    "    reviewPairs.append((reviewKeys[idx], reviewValues[idx]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Distribute the data\n",
    "reviews_rdd = spark.sparkContext.parallelize(reviewPairs, numSlices=8)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('XQfwVwDr-v0ZS3_CbbE5Xw',\n",
       "  \"If you decide to eat here, just be aware it is going to take about 2 hours from beginning to end. We have tried it multiple times, because I want to like it! I have been to it's other locations in NJ and never had a bad experience. \\n\\nThe food is good, but it takes a very long time to come out. The waitstaff is very young, but usually pleasant. We have just had too many experiences where we spent way too long waiting. We usually opt for another diner or restaurant on the weekends, in order to be done quicker.\")]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reviews_rdd.take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group pairs by the key\n",
    "groupedPairs = reviews_rdd.groupByKey()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "groupedPairs = groupedPairs.map(lambda x: (x[0], list(x[1])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('gmjsEdUsKpj9Xxu6pdjH0g',\n",
       "  [\"Loved this tour! I grabbed a groupon and the price was great. It was the perfect way to explore New Orleans for someone who'd never been there before and didn't know a lot about the history of the city. Our tour guide had tons of interesting tidbits about the city, and I really enjoyed the experience. Highly recommended tour. I actually thought we were just going to tour through the cemetery, but she took us around the French Quarter for the first hour, and the cemetery for the second half of the tour. You'll meet up in front of a grocery store (seems strange at first, but it's not terribly hard to find, and it'll give you a chance to get some water), and you'll stop at a visitor center part way through the tour for a bathroom break if needed. This tour was one of my favorite parts of my trip!\"]),\n",
       " ('EQ-TZ2eeD_E0BHuvoaeG5Q',\n",
       "  [\"Locals recommended Milktooth, and it's an amazing jewel of Indianapolis. I'm glade I had the chance to experience this.\",\n",
       "   \"Milktooth is the place to go if you want a good breakfast cocktail and a twist on traditional brunch. At first glance of the menu, you may think you've never heard of any of these foods for breakfast, and you may be right. I guarantee no matter what you order, you're going to enjoy it. \\n\\nMilktooth is one of the most popular brunch places in Indy, which means you will typically have a wait. Trust me, it's worth the usual 30 minute wait (on weekends). To make it go by a little faster, they've got a walk up bar for their coffee drinks and pastries. Snack on one of those to give you a preview of the amazing meal you're about to have.\\n\\nTheir menu changes from time to time but they have a few things that stay one the menu but change ingredients. I love all of their variations on a Dutch pancake, and their raclette grilled cheese is to die for.  If you're making this a boozy breakfast, don't pass up on the michelada. And sit outside if it's warm enough!\",\n",
       "   'Busy place, but we were offered extra- special coffee while we waited. No sooner than it was ready, so was our table. The server was welcoming and offered suggestions for us after we made her aware of some dietary restrictions. My daughter and I shared a salad, entree, and Special grits. Every bite was delicious, beautifully presented, and reasonably priced as downtown prices go. We like this restaurant a lot.']),\n",
       " ('vC2qm1y3Au5czBtbhc-DNw',\n",
       "  [\"Yes, this is the only sushi place in town. However, it is great when you're craving sushi and don't have time to go somewhere else. The salmon is probably the best fish they have, so we always order salmon. We also love their spicy edamame, tempura, ocean salad, and cabbage salad. Service has always been friendly and quick!\"]),\n",
       " ('MWmXGQ98KbRo3vsS5nZhMA',\n",
       "  [\"I recently had dinner here with my wife over the weekend and could not have been more pleased! \\n\\nOur meal was excellent! My wife and I were astounded by how quickly our food came out! Everything tasted fresh and homemade which we both appreciated. \\n\\nThe only problem was trying to pick something from the menu as there were too many delicious sounding items to choose from. We will definitely be returning to try more items! Can't wait to see what the next chef specials will be!\"]),\n",
       " ('ltBBYdNzkeKdCNPDAsxwAA',\n",
       "  ['I at least have to give this restaurant two stars due to the decent food. But while on a dinner meeting and after spending $100 on entrées and appetizers, I had to flag down the assistant cook, (the guy in bright red baggy pants watching TV), to see if I could get more avocado for my tuna entrée.  He made it very apparent and direct that he could not help me with that situation and I would have to flag down my  waitress so she could bill me for the four dollar avocado.'])]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "groupedPairs.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyzeBusiness(businessId, texts):\n",
    "    # restaurant[0] = business_id\n",
    "    # restaurant[1] = text\n",
    "    maximum_length=3\n",
    "    minimum_repeat=2\n",
    "    \n",
    "    phrases = {}\n",
    "    \n",
    "    for text in texts:\n",
    "        # clean text\n",
    "        text = re.sub(r'[.!?,:;/\\-\\s]',' ', text)\n",
    "        text = re.sub(r'[\\\\|@#$&~%\\(\\)*\\\"]', '', text)\n",
    "        \n",
    "        words = text.split(' ')\n",
    "        # filter stopwords\n",
    "        words = [w for w in words if len(w) and w.lower() not in stopwords]\n",
    "        length = len(words)\n",
    "        \n",
    "        # phrases can only be maximum_length words long\n",
    "        size = length if length <= maximum_length else maximum_length\n",
    "        while size > 0:\n",
    "            pos = 0\n",
    "            # walk over sets of words\n",
    "            while pos + size <= length:\n",
    "                phrase = words[pos:pos+size]\n",
    "                phrase = tuple(w.lower() for w in phrase)\n",
    "                if phrase in phrases:\n",
    "                    phrases[phrase] += 1\n",
    "                else:\n",
    "                    phrases[phrase] = 1\n",
    "                pos += 1\n",
    "            size -= 1\n",
    "    # remove phrases that are less than the length set by minimum_repeat\n",
    "    phrases = {k: v for k, v in phrases.items() if v >= minimum_repeat}\n",
    "    longest_phrases = {}\n",
    "    keys = list(phrases.keys())\n",
    "    keys.sort(key=len, reverse=True)\n",
    "    for phrase in keys:\n",
    "        found = False\n",
    "        for l_phrase in longest_phrases:\n",
    "            intersection = set(l_phrase).intersection(phrase)\n",
    "            if len(intersection) == len(phrase):\n",
    "                difference = float(phrases[phrase] - len(longest_phrases[l_phrase])) / len(longest_phrases[l_phrase])\n",
    "                if difference < 0.25:\n",
    "                    found = True\n",
    "                    break\n",
    "        if not found:\n",
    "            currentPhrase = ' '.join(phrase)\n",
    "            if currentPhrase in mapPhrases:\n",
    "                if currentPhrase not in longest_phrases:\n",
    "                    longest_phrases[mapPhrases[currentPhrase]] = mapValue[mapPhrases[currentPhrase]]\n",
    "    returnObj = dict()\n",
    "    returnObj['business_id'] = businessId\n",
    "    returnObj['pos'] = []\n",
    "    returnObj['neg'] = []\n",
    "    for phrase in longest_phrases:\n",
    "        if longest_phrases[phrase] == 'pos':\n",
    "            returnObj['pos'].append(phrase)\n",
    "        else:\n",
    "            returnObj['neg'].append(phrase)\n",
    "    return returnObj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "findProsAndCons = groupedPairs.map(lambda x: analyzeBusiness(x[0],x[1]))\n",
    "\n",
    "# final output\n",
    "# {\n",
    "#    business_id: string\n",
    "#    pros: []\n",
    "#    cons: []\n",
    "# }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# take output and store into a list\n",
    "examples = groupedPairs.take(10000)\n",
    "finalOutput = []\n",
    "for idx, example in enumerate(examples):\n",
    "    analyzeBusiness(example[0],example[1])\n",
    "    finalOutput.append(analyzeBusiness(example[0],example[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save output into json file\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_save_path = 'output/finalOutput.json'\n",
    "with open(output_save_path, 'w') as f:\n",
    "    json.dump(finalOutput, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_save_path = 'output/reviewID.txt'\n",
    "with open(output_save_path, 'w') as f:\n",
    "    for word in reviewKeys:\n",
    "        f.write('\"' + f\"{word}\" + '\",')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Results and Observations\n",
    "\n",
    "## Current Results and how to improve in the future\n",
    "- The results are decent, there are a good amount of pros and cons that are discovered by the filter and most restaurants have a decent list of pros and cons.\n",
    "- After getting the results there are clear limitations with this method.\n",
    "- First, due to only taking a subset of the data, there are many instances where some businesses have very few reviews, therefore those businesses may get little to no pros and cons. If that small handful of reviews do not contain any of the common phrases then the pros and cons arrays will be empty. As a default, if there are no pros and cons found, the attributes row of the business works well as a default pros and cons array. Attributes contain key,value pairs of business attributes such as if the restaurant has take out, what type of parking, etc. A solution to this issue is to increase the subset size, but there was not enough time to test what is the maximum size subset we can use on our machine.\n",
    "- This filter is general, because the function iterates through review texts of all restaurants, the most common phrases will be phrases that are shared across all reviews. Therefore, specific pros and cons that may only apply to that restaurant in particular may not be caught. A solution to this is to find pros and cons of each type of restaurant at a time(burger, sushi, etc.) this way, we can get more specific pros and cons.\n",
    "- Cons are harder to detect than pros. Reviews who have negative things to say about a restaurant tend to be more descriptive and specific about their issue, this leads to less negative common phrases than positive ones. I believe a machine learning method to classify the pros and cons may alleviate this issue.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py312",
   "language": "python",
   "name": "py312"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
